{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CKYuLSS6ZOgO",
        "p7vrYGuQcdzi",
        "9yS8_7VZdsQ0",
        "fK-npo-na15Q",
        "fyvvF7P_hW21"
      ],
      "authorship_tag": "ABX9TyOCFUXKcZz/OjJk3mjnn3kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lastinm/mlops_practice/blob/main/%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%961_Apache_Spark_(MapReduce).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек из PySpark"
      ],
      "metadata": {
        "id": "CKYuLSS6ZOgO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ijX5-NiWXJB8"
      },
      "outputs": [],
      "source": [
        "# Импортируем необходимые библиотеки\n",
        "from pyspark import SparkConf, SparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Для работы с текстовым файлом подключим google диск."
      ],
      "metadata": {
        "id": "FzqY1qWfZUSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Подключаем Google Drive\n",
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRJuboDhXXcm",
        "outputId": "a20faa49-dd1b-4806-f0e6-39395bcd4ebe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1\n",
        "\n",
        "Используя Apache Spark (MapReduce), реализовать программу для подсчета количества слов в тексте, который представляет собой некоторое количество строк, единственный разделитель между словами – пробел."
      ],
      "metadata": {
        "id": "p7vrYGuQcdzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Решение"
      ],
      "metadata": {
        "id": "9yS8_7VZdsQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*MapReduce в контексте Spark*\n",
        "\n",
        "**MapReduce** — это модель программирования, которая изначально была предложена Google для обработки и генерации больших наборов данных. Она разделяет задачу обработки на две основные фазы:\n",
        "\n",
        "**Map**: Данные обрабатываются функцией Map, которая берет набор данных и преобразует его в набор пар \"ключ-значение\".\n",
        "**Reduce**: Затем функция Reduce принимает эти пары ключей и значений, группирует их и производит итоговые результаты."
      ],
      "metadata": {
        "id": "7zqezYS7dk8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных\n",
        "\n",
        "Есть текстовый файл со сказкой А.С. Пушкина \"Золотая рыбка\", содержащий строки текста. Мы будем загружать содержимое этого файла в RDD."
      ],
      "metadata": {
        "id": "fK-npo-na15Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем текстовый файл\n",
        "text_file = sc.textFile(\"/content/drive/MyDrive/Data/Пушкин А.С. - Сказка о рыбаке и рыбке.txt\")"
      ],
      "metadata": {
        "id": "C7hiGp_PbKPZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Инициализация SparkContext\n",
        "\n",
        "Инициализируем контекст Spark. Это основа для работы с RDD (Resilient Distributed Dataset)."
      ],
      "metadata": {
        "id": "wAV54GNRanlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подсчет слов"
      ],
      "metadata": {
        "id": "fyvvF7P_hW21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем конфигурацию Spark\n",
        "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "etZnUIpVaHet"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для понимания, как работает Spark, рассмотрим простой пример подсчета слов, который можно реализовать с использованием концепций MapReduce.\n",
        "\n",
        "Map: Преобразуем текст в слова, создавая пары \"слово, 1\".\n",
        "\n",
        "Reduce: Суммируем значения для каждого слова, чтобы получить его общее количество."
      ],
      "metadata": {
        "id": "lBvcLsA2eaL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразуем текст в слова\n",
        "words = text_file.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Подсчитываем количество слов\n",
        "list_words = words.map(lambda word: (word, 1))\n",
        "list_words.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vVlTFG4byg1",
        "outputId": "6a4c53ce-51b8-4a17-e484-ae60c7010ede"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Жил', 1), ('старик', 1), ('со', 1), ('своею', 1), ('старухой', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = list_words.reduceByKey(lambda a, b: a + b)\n",
        "word_counts.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hoqHX1me1LM",
        "outputId": "ce018a65-b401-45d3-804b-d6017749f187"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Жил', 1),\n",
              " ('старухой', 1),\n",
              " ('У', 2),\n",
              " ('самого', 1),\n",
              " ('синего', 1),\n",
              " ('моря;', 1),\n",
              " ('Они', 1),\n",
              " ('землянке', 1),\n",
              " ('Ровно', 1),\n",
              " ('тридцать', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "\n",
        "Используя Apache Spark (DataFrame), реализовать программу для подсчета количества слов в тексте, которые представляет собой некоторое количество строк, единственный разделитель между словами – пробел."
      ],
      "metadata": {
        "id": "XlTK-R2vf8tB"
      }
    }
  ]
}